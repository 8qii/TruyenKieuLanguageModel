{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b960fd95-65e7-4a8f-b907-70523994151f",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ddf124-9961-404a-9140-9e7260289241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f27259c-d500-4c31-8a06-a2d6ea261381",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "block_size = 64\n",
    "max_iters = 5000\n",
    "eval_iters = 100\n",
    "n_embd = 384\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "learning_rate = 3e-4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46cf57bf-0752-4bc7-b1c6-49c58cd2edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4b974f-e937-4e83-8141-8140ad875c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phần 1:\n",
      "Trăm năm trong cõi người ta,\n",
      "Chữ tài chữ mệnh khéo là ghét nhau.\n",
      "Trải qua một cuộc bể dâu,\n",
      "Những điều trông thấy mà đau đớn lòng.\n",
      "Lạ gì bỉ sắc\n"
     ]
    }
   ],
   "source": [
    "with open('data/truyenkieu.txt', 'r', encoding = 'utf-8') as f:\n",
    "    data = f.read()\n",
    "print(data[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a33e6f-b035-4810-ad5d-f4c62ac74c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['\\n', ' ', '!', ',', '-', '.', '1', '2', '3', '4', ':', '?', 'A', 'B', 'C', 'D', 'E', 'G', 'H', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'Â', 'Ê', 'Ô', 'à', 'á', 'â', 'ã', 'è', 'é', 'ê', 'ì', 'í', 'ò', 'ó', 'ô', 'õ', 'ù', 'ú', 'ý', 'Ă', 'ă', 'Đ', 'đ', 'ĩ', 'ũ', 'Ơ', 'ơ', 'Ư', 'ư', 'ạ', 'ả', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ẹ', 'ẻ', 'ẽ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ỉ', 'ị', 'ọ', 'ỏ', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'ụ', 'ủ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ỳ', 'ỷ', 'ỹ']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(data))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c4456-4cbc-4bf4-8658-fb3fdcdb06cd",
   "metadata": {},
   "source": [
    "### Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303fb868-6905-4014-8ef4-a8e6ab4f69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "string_to_int = { ch:i for i, ch in enumerate(vocab)}\n",
    "int_to_string = { i:ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda i: ''.join(int_to_string[c] for c in i)\n",
    "\n",
    "data = torch.tensor(encode(data), dtype=torch.long)\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a39ae9-c6c7-4a43-8277-b3a1137b6a44",
   "metadata": {},
   "source": [
    "### Get batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40042a8-9240-441c-b751-acb24ece8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = int(0.9 * len(data))\n",
    "train_split = data[:pos]\n",
    "val_split = data[pos:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data_set = train_split if split == 'train' else val_split\n",
    "    num = torch.randint(len(data_set) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_set[i: i + block_size] for i in num])\n",
    "    y = torch.stack([data_set[i + 1: i + block_size + 1] for i in num])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36403d6f-a190-4762-9cde-d375ec16fe24",
   "metadata": {},
   "source": [
    "### Your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3f4fd0-c3b3-4c5f-85ce-fd677ea14dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        k = self.key(x) # B, T, Hs\n",
    "        q = self.query(x) # B, T, Hs\n",
    "\n",
    "        dot_prod = k @ q.transpose(-2, -1) # --> B, T, T\n",
    "        wei = dot_prod * k.shape[-1] ** -0.5 # --> B, T, T\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out \n",
    "        \n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_head * head_size , n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out \n",
    "\n",
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head,head_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "class TruyenKieuLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.decoder_blocks = nn.Sequential(*[Decoder_Block(n_embd, n_head = n_head) for i in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "\n",
    "    def forward(self, index, targets = None):\n",
    "        B, T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.decoder_blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            index_cond = index[:, -block_size:]\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            \n",
    "            logits = logits[:, -1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            index_next = torch.multinomial(probs, num_samples = 1)\n",
    "            index = torch.cat((index, index_next) , dim = 1)\n",
    "            \n",
    "        return index\n",
    "model = TruyenKieuLanguageModel(vocab_size)\n",
    "# print('loading model parameters...')\n",
    "# with open('model-01.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)\n",
    "# print('loaded successfully!')\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f7fce-3d68-4112-8141-e3c7b6932ebf",
   "metadata": {},
   "source": [
    "### Loss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2d2d58f-8ca8-4dcd-a277-52d2f630722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def loss_detail():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'validate']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21670869-95f5-43c6-a765-f5ee0d1567b0",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5696bc14-2ce7-47bb-81d1-ced477aed877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Can you see me??ổặDẵẫDâẻBểìETkàâẻKsàí1YũQỉđpsỷqPằlxlùưéeéổ\n",
      "\n",
      "òãOtễdHTsằyƯ:ứnƠởủÂeÊễứĐípồgo-Ướẵ-í1íểữộdởlỉ1ữủễơOOcdMỷ\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Hello! Can you see me?'\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37fabdef-15bc-4299-8c32-8d5e4476b8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vòng huấn luyện: 0, loss: {'train': tensor(4.9265), 'validate': tensor(4.9303)}\n",
      "Vòng huấn luyện: 50\n",
      "Vòng huấn luyện: 100, loss: {'train': tensor(2.0804), 'validate': tensor(2.0988)}\n",
      "Vòng huấn luyện: 150\n",
      "Vòng huấn luyện: 200, loss: {'train': tensor(1.8399), 'validate': tensor(1.8692)}\n",
      "Vòng huấn luyện: 250\n",
      "Vòng huấn luyện: 300, loss: {'train': tensor(1.7218), 'validate': tensor(1.7516)}\n",
      "Vòng huấn luyện: 350\n",
      "Vòng huấn luyện: 400, loss: {'train': tensor(1.6414), 'validate': tensor(1.6856)}\n",
      "Vòng huấn luyện: 450\n",
      "Vòng huấn luyện: 500, loss: {'train': tensor(1.5899), 'validate': tensor(1.6545)}\n",
      "Vòng huấn luyện: 550\n",
      "Vòng huấn luyện: 600, loss: {'train': tensor(1.5381), 'validate': tensor(1.6257)}\n",
      "Vòng huấn luyện: 650\n",
      "Vòng huấn luyện: 700, loss: {'train': tensor(1.4897), 'validate': tensor(1.6118)}\n",
      "Vòng huấn luyện: 750\n",
      "Vòng huấn luyện: 800, loss: {'train': tensor(1.4441), 'validate': tensor(1.6065)}\n",
      "Vòng huấn luyện: 850\n",
      "Vòng huấn luyện: 900, loss: {'train': tensor(1.3899), 'validate': tensor(1.5981)}\n",
      "Vòng huấn luyện: 950\n",
      "Vòng huấn luyện: 1000, loss: {'train': tensor(1.3296), 'validate': tensor(1.5955)}\n",
      "Vòng huấn luyện: 1050\n",
      "Vòng huấn luyện: 1100, loss: {'train': tensor(1.2745), 'validate': tensor(1.6041)}\n",
      "Vòng huấn luyện: 1150\n",
      "Vòng huấn luyện: 1200, loss: {'train': tensor(1.2101), 'validate': tensor(1.6225)}\n",
      "Vòng huấn luyện: 1250\n",
      "Vòng huấn luyện: 1300, loss: {'train': tensor(1.1411), 'validate': tensor(1.6384)}\n",
      "Vòng huấn luyện: 1350\n",
      "Vòng huấn luyện: 1400, loss: {'train': tensor(1.0748), 'validate': tensor(1.6591)}\n",
      "Vòng huấn luyện: 1450\n",
      "Vòng huấn luyện: 1500, loss: {'train': tensor(0.9987), 'validate': tensor(1.7046)}\n",
      "Vòng huấn luyện: 1550\n",
      "Vòng huấn luyện: 1600, loss: {'train': tensor(0.9215), 'validate': tensor(1.7181)}\n",
      "Vòng huấn luyện: 1650\n",
      "Vòng huấn luyện: 1700, loss: {'train': tensor(0.8501), 'validate': tensor(1.7622)}\n",
      "Vòng huấn luyện: 1750\n",
      "Vòng huấn luyện: 1800, loss: {'train': tensor(0.7788), 'validate': tensor(1.8111)}\n",
      "Vòng huấn luyện: 1850\n",
      "Vòng huấn luyện: 1900, loss: {'train': tensor(0.7142), 'validate': tensor(1.8590)}\n",
      "Vòng huấn luyện: 1950\n",
      "Vòng huấn luyện: 2000, loss: {'train': tensor(0.6517), 'validate': tensor(1.9112)}\n",
      "Vòng huấn luyện: 2050\n",
      "Vòng huấn luyện: 2100, loss: {'train': tensor(0.5861), 'validate': tensor(1.9630)}\n",
      "Vòng huấn luyện: 2150\n",
      "Vòng huấn luyện: 2200, loss: {'train': tensor(0.5382), 'validate': tensor(2.0241)}\n",
      "Vòng huấn luyện: 2250\n",
      "Vòng huấn luyện: 2300, loss: {'train': tensor(0.4890), 'validate': tensor(2.0616)}\n",
      "Vòng huấn luyện: 2350\n",
      "Vòng huấn luyện: 2400, loss: {'train': tensor(0.4448), 'validate': tensor(2.1364)}\n",
      "Vòng huấn luyện: 2450\n",
      "Vòng huấn luyện: 2500, loss: {'train': tensor(0.4084), 'validate': tensor(2.1727)}\n",
      "Vòng huấn luyện: 2550\n",
      "Vòng huấn luyện: 2600, loss: {'train': tensor(0.3707), 'validate': tensor(2.2461)}\n",
      "Vòng huấn luyện: 2650\n",
      "Vòng huấn luyện: 2700, loss: {'train': tensor(0.3409), 'validate': tensor(2.2935)}\n",
      "Vòng huấn luyện: 2750\n",
      "Vòng huấn luyện: 2800, loss: {'train': tensor(0.3182), 'validate': tensor(2.3513)}\n",
      "Vòng huấn luyện: 2850\n",
      "Vòng huấn luyện: 2900, loss: {'train': tensor(0.2965), 'validate': tensor(2.4228)}\n",
      "Vòng huấn luyện: 2950\n",
      "Vòng huấn luyện: 3000, loss: {'train': tensor(0.2785), 'validate': tensor(2.4307)}\n",
      "Vòng huấn luyện: 3050\n",
      "Vòng huấn luyện: 3100, loss: {'train': tensor(0.2594), 'validate': tensor(2.4995)}\n",
      "Vòng huấn luyện: 3150\n",
      "Vòng huấn luyện: 3200, loss: {'train': tensor(0.2483), 'validate': tensor(2.5407)}\n",
      "Vòng huấn luyện: 3250\n",
      "Vòng huấn luyện: 3300, loss: {'train': tensor(0.2396), 'validate': tensor(2.5922)}\n",
      "Vòng huấn luyện: 3350\n",
      "Vòng huấn luyện: 3400, loss: {'train': tensor(0.2310), 'validate': tensor(2.6707)}\n",
      "Vòng huấn luyện: 3450\n",
      "Vòng huấn luyện: 3500, loss: {'train': tensor(0.2225), 'validate': tensor(2.6791)}\n",
      "Vòng huấn luyện: 3550\n",
      "Vòng huấn luyện: 3600, loss: {'train': tensor(0.2160), 'validate': tensor(2.7408)}\n",
      "Vòng huấn luyện: 3650\n",
      "Vòng huấn luyện: 3700, loss: {'train': tensor(0.2134), 'validate': tensor(2.7675)}\n",
      "Vòng huấn luyện: 3750\n",
      "Vòng huấn luyện: 3800, loss: {'train': tensor(0.2064), 'validate': tensor(2.7949)}\n",
      "Vòng huấn luyện: 3850\n",
      "Vòng huấn luyện: 3900, loss: {'train': tensor(0.2036), 'validate': tensor(2.8472)}\n",
      "Vòng huấn luyện: 3950\n",
      "Vòng huấn luyện: 4000, loss: {'train': tensor(0.1991), 'validate': tensor(2.8687)}\n",
      "Vòng huấn luyện: 4050\n",
      "Vòng huấn luyện: 4100, loss: {'train': tensor(0.1972), 'validate': tensor(2.8952)}\n",
      "Vòng huấn luyện: 4150\n",
      "Vòng huấn luyện: 4200, loss: {'train': tensor(0.1949), 'validate': tensor(2.9647)}\n",
      "Vòng huấn luyện: 4250\n",
      "Vòng huấn luyện: 4300, loss: {'train': tensor(0.1909), 'validate': tensor(3.0096)}\n",
      "Vòng huấn luyện: 4350\n",
      "Vòng huấn luyện: 4400, loss: {'train': tensor(0.1898), 'validate': tensor(2.9903)}\n",
      "Vòng huấn luyện: 4450\n",
      "Vòng huấn luyện: 4500, loss: {'train': tensor(0.1869), 'validate': tensor(3.0604)}\n",
      "Vòng huấn luyện: 4550\n",
      "Vòng huấn luyện: 4600, loss: {'train': tensor(0.1862), 'validate': tensor(3.0600)}\n",
      "Vòng huấn luyện: 4650\n",
      "Vòng huấn luyện: 4700, loss: {'train': tensor(0.1849), 'validate': tensor(3.0869)}\n",
      "Vòng huấn luyện: 4750\n",
      "Vòng huấn luyện: 4800, loss: {'train': tensor(0.1829), 'validate': tensor(3.1700)}\n",
      "Vòng huấn luyện: 4850\n",
      "Vòng huấn luyện: 4900, loss: {'train': tensor(0.1817), 'validate': tensor(3.1432)}\n",
      "Vòng huấn luyện: 4950\n",
      "Vòng huấn luyện: 5000, loss: {'train': tensor(0.1796), 'validate': tensor(3.1604)}\n",
      "Vòng huấn luyện: 5050\n",
      "Vòng huấn luyện: 5100, loss: {'train': tensor(0.1792), 'validate': tensor(3.1904)}\n",
      "Vòng huấn luyện: 5150\n",
      "Vòng huấn luyện: 5200, loss: {'train': tensor(0.1792), 'validate': tensor(3.2155)}\n",
      "Vòng huấn luyện: 5250\n",
      "Vòng huấn luyện: 5300, loss: {'train': tensor(0.1755), 'validate': tensor(3.2469)}\n",
      "Vòng huấn luyện: 5350\n",
      "Vòng huấn luyện: 5400, loss: {'train': tensor(0.1752), 'validate': tensor(3.2631)}\n",
      "Vòng huấn luyện: 5450\n",
      "Vòng huấn luyện: 5500, loss: {'train': tensor(0.1750), 'validate': tensor(3.3054)}\n",
      "Vòng huấn luyện: 5550\n",
      "Vòng huấn luyện: 5600, loss: {'train': tensor(0.1747), 'validate': tensor(3.3084)}\n",
      "Vòng huấn luyện: 5650\n",
      "Vòng huấn luyện: 5700, loss: {'train': tensor(0.1746), 'validate': tensor(3.2914)}\n",
      "Vòng huấn luyện: 5750\n",
      "Vòng huấn luyện: 5800, loss: {'train': tensor(0.1722), 'validate': tensor(3.3675)}\n",
      "Vòng huấn luyện: 5850\n",
      "Vòng huấn luyện: 5900, loss: {'train': tensor(0.1723), 'validate': tensor(3.3331)}\n",
      "Vòng huấn luyện: 5950\n",
      "Vòng huấn luyện: 6000, loss: {'train': tensor(0.1714), 'validate': tensor(3.3926)}\n",
      "Vòng huấn luyện: 6050\n",
      "Vòng huấn luyện: 6100, loss: {'train': tensor(0.1701), 'validate': tensor(3.3927)}\n",
      "Vòng huấn luyện: 6150\n",
      "Vòng huấn luyện: 6200, loss: {'train': tensor(0.1720), 'validate': tensor(3.4223)}\n",
      "Vòng huấn luyện: 6250\n",
      "Vòng huấn luyện: 6300, loss: {'train': tensor(0.1687), 'validate': tensor(3.4627)}\n",
      "Vòng huấn luyện: 6350\n",
      "Vòng huấn luyện: 6400, loss: {'train': tensor(0.1690), 'validate': tensor(3.4665)}\n",
      "Vòng huấn luyện: 6450\n",
      "Vòng huấn luyện: 6500, loss: {'train': tensor(0.1682), 'validate': tensor(3.4820)}\n",
      "Vòng huấn luyện: 6550\n",
      "Vòng huấn luyện: 6600, loss: {'train': tensor(0.1670), 'validate': tensor(3.4874)}\n",
      "Vòng huấn luyện: 6650\n",
      "Vòng huấn luyện: 6700, loss: {'train': tensor(0.1677), 'validate': tensor(3.4633)}\n",
      "Vòng huấn luyện: 6750\n",
      "Vòng huấn luyện: 6800, loss: {'train': tensor(0.1665), 'validate': tensor(3.5578)}\n",
      "Vòng huấn luyện: 6850\n",
      "Vòng huấn luyện: 6900, loss: {'train': tensor(0.1658), 'validate': tensor(3.5640)}\n",
      "Vòng huấn luyện: 6950\n",
      "Vòng huấn luyện: 7000, loss: {'train': tensor(0.1662), 'validate': tensor(3.5610)}\n",
      "Vòng huấn luyện: 7050\n",
      "Vòng huấn luyện: 7100, loss: {'train': tensor(0.1653), 'validate': tensor(3.6065)}\n",
      "Vòng huấn luyện: 7150\n",
      "Vòng huấn luyện: 7200, loss: {'train': tensor(0.1655), 'validate': tensor(3.5700)}\n",
      "Vòng huấn luyện: 7250\n",
      "Vòng huấn luyện: 7300, loss: {'train': tensor(0.1638), 'validate': tensor(3.5964)}\n",
      "Vòng huấn luyện: 7350\n",
      "Vòng huấn luyện: 7400, loss: {'train': tensor(0.1656), 'validate': tensor(3.5900)}\n",
      "Vòng huấn luyện: 7450\n",
      "Vòng huấn luyện: 7500, loss: {'train': tensor(0.1646), 'validate': tensor(3.6304)}\n",
      "Vòng huấn luyện: 7550\n",
      "Vòng huấn luyện: 7600, loss: {'train': tensor(0.1631), 'validate': tensor(3.6488)}\n",
      "Vòng huấn luyện: 7650\n",
      "Vòng huấn luyện: 7700, loss: {'train': tensor(0.1622), 'validate': tensor(3.6201)}\n",
      "Vòng huấn luyện: 7750\n",
      "Vòng huấn luyện: 7800, loss: {'train': tensor(0.1625), 'validate': tensor(3.6985)}\n",
      "Vòng huấn luyện: 7850\n",
      "Vòng huấn luyện: 7900, loss: {'train': tensor(0.1624), 'validate': tensor(3.6816)}\n",
      "Vòng huấn luyện: 7950\n",
      "0.23498423397541046\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "for iter in range (max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = loss_detail()\n",
    "        print(f'Vòng huấn luyện: {iter}, loss: {losses}')\n",
    "    elif iter % 50 == 0:\n",
    "        print(f'Vòng huấn luyện: {iter}')\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "755c09d1-6efa-453d-93a1-2e65a6cdbcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b44d4eea-07a0-4e48-b2bf-fdca6bc24062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thúy Kiều là ai thị tì tao!\n",
      "Rồi ra trở mặt tức thì,\n",
      "Bớt lời liệu chớ sân si thiệt đời!\n",
      "Nàng rằng: Thề thốt nặng lời,\n",
      "Có đâu mà chẳng dám ngăn rời chim xanh.\n",
      "Đã cho vào bậc bố bề lửa dong,\n",
      "Rước mời vào thằng bán tơ!\n",
      "Một nhà đạp nận khan gâm.\n",
      "Cớ nhà hương mở mây tạnh thong dong,\n",
      "Thang lan rủ bức hồng trưa,\n",
      "Đóng thuê thoắt đã đổi thay mang lấy sắc tài e châm.\n",
      "Vì ta rong theo tiết gọng mái tường,\n",
      "Dàu dàu ngọn cỏ nửa vàng nửa xanh.\n",
      "Rằng: Sao trong tiết Thanh minh,\n",
      "Mà đây hương tuốt thiên tầm như gia.\n",
      "Được rày như nhọ Vương người dở dang.\n",
      "Thề hoa chưa ráù thẹn phím nàng ngài kíp vu quan.\n",
      "Vộc lời chi vui chén thong dong,\n",
      "Nỗi lòng ai ở trong lòng mà ra tra.\n",
      "Lại càng ủ dột tỉnh lúc mê,\n",
      "Máu theo nước mắt hoa lòng trên duy.\n",
      "Sóng song the với càng,\n",
      "Nghĩ dày càng tỏ trang tự tình.\n",
      "Gót đầu mới trướng hồng vững lời đâu?\n",
      "Sân Lai càng như dại nhớ ra,\n",
      "Chiều duyên chưa dứt thì bằn chị trao.\n",
      "Hoa khôi lỡ một lầm hồng xuống môn.\n",
      "Nghĩ người thôi lại nghĩ mình,\n",
      "Cảm lòa cho mặt chữ tình cho nên!\n",
      "Trên mui lăng một thiếp người\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Thúy Kiều là ai'\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=1000)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
